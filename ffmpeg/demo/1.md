```cpp
// FFmpeg.cpp : 定义控制台应用程序的入口点。
//

#include "stdafx.h"
#include <libavformat/internal.h>

#define MAX_AUDIO_FRAME_SIZE 192000 // 1 second of 48khz 32bit audio

//SDL beign
//Refresh Event
#define SFM_REFRESH_EVENT (SDL_USEREVENT + 1)
#define SFM_BREAK_EVENT (SDL_USEREVENT + 2)

int thread_exit = 0;
int thread_pause = 0;

int sfp_refresh_thread(void *opaque)
{
    thread_exit = 0;
    thread_pause = 0;

    while(!thread_exit)
    {
        if(!thread_pause)
        {
            SDL_Event event;
            event.type = SFM_REFRESH_EVENT;
            SDL_PushEvent(&event);
        }
        SDL_Delay(1);
    }
    thread_exit = 0;
    thread_pause = 0;
    //break
    SDL_Event event;
    event.type = SFM_BREAK_EVENT;
    SDL_PushEvent(&event);
    return 0;
}
//SDL end

//Audio begin
static Uint8 *audio_chunk;
static Uint32 audio_len;
static Uint8 *audio_pos;
/* The audio function callback takes the following parameters:  
* stream: A pointer to the audio buffer to be filled  
* len: The length (in bytes) of the audio buffer  
*/
//Buffer:  
//|-----------|-------------|  
//chunk-------pos---len-----|
void  fill_audio(void *udata,Uint8 *stream,int len){   
    //SDL 2.0  
    SDL_memset(stream, 0, len);  
    if(audio_len==0)  
        return;   

    len=(len>audio_len?audio_len:len);   /*  Mix  as  much  data  as  possible  */   

    SDL_MixAudio(stream,audio_pos,len,SDL_MIX_MAXVOLUME);  
    audio_pos += len;   
    audio_len -= len;   
} 
//Audio end

void get_dsshow_device(AVFormatContext *pFormatCtx)
{
    AVDictionary *options = NULL;
    av_dict_set(&options,"list_devices","true",0);
    AVInputFormat *iformat = av_find_input_format("dshow");  
    printf("Device Info=============\n");  
    avformat_open_input(&pFormatCtx,"video=dummy",iformat,&options);  
    printf("========================\n");  

}

enum {
    FLV_TAG_TYPE_AUDIO = 0x08,
    FLV_TAG_TYPE_VIDEO = 0x09,
    FLV_TAG_TYPE_META  = 0x12,
};

int get_video_extradata(AVFormatContext *s, int video_index)
{
    int  type, size;
    int ret = -1;
    //int64_t dts;
    bool got_extradata = false;

    if (!s || video_index < 0 || video_index > 2)
        return ret;

    //Skip Previous Tag Size
    for (;; avio_skip(s->pb, 4)) {
        avio_tell(s->pb);
        /*FLV TAG:  
        **|-8--|-24-|----24---|-----8------|---24----|----|
        **|type|size|timestamp|timestamp ex|Stream ID|Data|
        **1.type: TAG中第1个字节中的前5位表示这个TAG中包含数据的类型,8 = audio,9 = video,18 = script data.
        **2.size:Stream ID之后的数据长度.
        **3.timestamp和timestampExtended组成了这个TAG包数据的PTS信息,记得刚开始做FVL demux的时候，
        **并没有考虑TimestampExtended的值,直接就把Timestamp默认为是PTS，后来发生的现 象就是画面有跳帧的现象,
        **后来才仔细看了一下文档发现真正数据的PTS是PTS= Timestamp | TimestampExtended<<24.
        **4.StreamID之后的数据就是每种格式的情况不一样了，接下格式进行详细的介绍.
        **  http://blog.csdn.net/leixiaohua1020/article/details/17934487    
        **  http://www.cnblogs.com/musicfans/archive/2012/11/07/2819291.html
        */
        type = avio_r8(s->pb);
        size = avio_rb24(s->pb);
        avio_skip(s->pb, 7);//Skip TimeStamp && Ex && Stream ID(always zero).
        //dts  = avio_rb24(s->pb);
        //dts |= avio_r8(s->pb) << 24;
        //avio_skip(s->pb, 3);//Skip Stream ID, always zero.

        if (0 == size)
            break;
        if (FLV_TAG_TYPE_AUDIO == type || FLV_TAG_TYPE_META == type) {
            //if audio or meta tags, skip them.
            avio_seek(s->pb, size, SEEK_CUR);
        }else if (type == FLV_TAG_TYPE_VIDEO) {
            printf("find video flv tag success %d.\n",size);
            //if the first video tag, read the sps/pps info from it. then break.
            //For H264(avc),skip 5!
            size -= 5;
            s->streams[video_index]->codec->extradata = (uint8_t *)av_mallocz(size + FF_INPUT_BUFFER_PADDING_SIZE);
            if (NULL == s->streams[video_index]->codec->extradata)
                break;
            memset(s->streams[video_index]->codec->extradata, 0, size + FF_INPUT_BUFFER_PADDING_SIZE);
            memcpy(s->streams[video_index]->codec->extradata, s->pb->buf_ptr + 5, size);
            s->streams[video_index]->codec->extradata_size = size;
            /***add for new API begin***/
            s->streams[video_index]->codecpar->extradata = (uint8_t *)av_mallocz(size + FF_INPUT_BUFFER_PADDING_SIZE);
            if (NULL == s->streams[video_index]->codecpar->extradata)
                break;
            memset(s->streams[video_index]->codecpar->extradata, 0, size + FF_INPUT_BUFFER_PADDING_SIZE);
            memcpy(s->streams[video_index]->codecpar->extradata, s->pb->buf_ptr + 5, size);
            s->streams[video_index]->codecpar->extradata_size = size;
            /***add for new API end***/
            ret = 0;
            got_extradata = true;
        } else  {
            break;
        }

        if (got_extradata)
            break;
    }

    return ret;
}

static AVPacket *add_to_pktbuf(AVPacketList **packet_buffer, AVPacket *pkt, AVPacketList **plast_pktl)
{
    AVPacketList *pktl = (AVPacketList *)av_mallocz(sizeof(AVPacketList));
    if (!pktl)
        return NULL;

    if (*packet_buffer)
        (*plast_pktl)->next = pktl;
    else
        *packet_buffer = pktl;

    /* Add the packet in the buffered packet list. */
    *plast_pktl = pktl;
    pktl->pkt   = *pkt;
    return &pktl->pkt;
}
//Video:        ffmpeg -f gdigrab -video_size 640x480 -i desktop  -vcodec libx264 -preset:v ultrafast -pix_fmt yuv420p -tune:v zerolatency -f flv rtmp://192.168.1.201/live/mystream
//Audio+Video:    ffmpeg -f dshow -i audio="FrontMic (Realtek High Definition Audio)" -f gdigrab -video_size 640x480 -i desktop  -vcodec libx264 -codec:a aac -pix_fmt yuv420p -tune zerolatency -preset ultrafast -f flv rtmp://192.168.1.201/live/mystream
void init_Stream1(AVFormatContext *formatCtx, int64_t start)
{
    /*
    * 根据读取Packet方式来自动获取一些参数信息：Video+Audio
    * 如果只有Audio或者Video则优化很明显，但是当同时存在时，获取时间会大幅度提升，基于此方案，
    * 可以通过提高首个视频I帧的时机来达到优化效果。理论上来说，只要打开流时，能快速获取第一个
    * 视频包+音频包则可以达到优化效果。
    */
    //const int genpts = formatCtx->flags & AVFMT_FLAG_GENPTS;
    //formatCtx->flags |= AVFMT_FLAG_GENPTS;

    AVPacket packet;
    av_init_packet(&packet);
    while (true)
    {
        int ret1 = av_read_frame(formatCtx, &packet);
        if (packet.flags & AV_PKT_FLAG_KEY)
        {
            if(formatCtx->nb_streams == 1 && formatCtx->streams[0]->codec->codec_type == AVMEDIA_TYPE_AUDIO){
                //we must get video stream!
                continue;
            }
            printf("Find I Frame here %d \n",formatCtx->nb_streams);
            break;
        }
    }
    printf("==========>begin find stream info 2: %0.3fs\n",(av_gettime()-start) / 1000000.0 );
    //void* pbs = &formatCtx->internal;
    //void* rpbs = &formatCtx->internal->packet_buffer;
    add_to_pktbuf(&formatCtx->internal->packet_buffer,&packet,&formatCtx->internal->packet_buffer_end);

    int videoIndex = 0, audioIndex = 0, i = 0;
    for(i = 0; i < formatCtx->nb_streams; i++)
    {
        if(formatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO)
        {
            videoIndex = i;
        }else if(formatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO){
            audioIndex = i;
        }
    }

    /***************************** Video ********************************/
    //formatCtx->streams[0] = st;
    AVStream *st = formatCtx->streams[videoIndex];
    //Init the video codec(H264).
    //st->codec->codec_id = AV_CODEC_ID_H264;
    st->codec->width = 640;
    st->codec->height = 480;
    //st->codec->ticks_per_frame = 2;
    st->codec->pix_fmt = AV_PIX_FMT_YUV420P;//AV_PIX_FMT_YUV420P;
    //st->pts_wrap_bits = 32;
    st->codec->time_base.den = st->time_base.den;
    st->codec->time_base.num = st->time_base.num;
    //st->codec->sample_fmt = AV_SAMPLE_FMT_NONE;
    //st->codec->codec_type = AVMEDIA_TYPE_VIDEO;
    //st->time_base.den = 1000;
    //st->time_base.num = 1;
    st->r_frame_rate.den = st->avg_frame_rate.den;
    st->r_frame_rate.num = st->avg_frame_rate.num;
    //st->avg_frame_rate.den = 2;
    //st->avg_frame_rate.num = 60;

    //empty the buffer.
    // H264 need sps/pps for decoding, so read it from the first video tag.
    //printf("get_video_extradata ret = %d\n",get_video_extradata(formatCtx, 0));
    //formatCtx->pb->buf_ptr = formatCtx->pb->buf_end;
    //formatCtx->pb->pos = (int64_t) formatCtx->pb->buf_end;

    /***************************** Audio ********************************/
    AVStream *at = formatCtx->streams[audioIndex];
    //Audio: aac (LC) ([10][0][0][0] / 0x000A), 44100 Hz, stereo, fltp, 128 kb/s
    //at->codec->sample_rate = 8000;//44100
    at->codec->time_base.den = 44100;
    at->codec->time_base.num = 1;
    //at->codec->bits_per_coded_sample = 16; //
    //at->codec->channels = 1;2
    //at->codec->channel_layout = 4;3
    //at->codec->codec_type = AVMEDIA_TYPE_AUDIO;
    //at->codec->bit_rate = 16000;//12800
    //at->codec->refs = 1;
    at->codec->sample_fmt = AV_SAMPLE_FMT_FLTP;
    at->codec->profile = 1;
    //at->codec->level = -99;

    //at->pts_wrap_bits = 32;
    //at->time_base.den = 1000;
    //at->time_base.num = 1;
}

void init_Stream2(AVFormatContext *formatCtx, int64_t start)
{
    /**
    * 完全自己构建音视频流信息,次方法严重依赖第一个flv video tag的读取速度。
    * 如果想提速，可以考虑通过其它方式传输extradata信息，来实现真正优化。
    **/
    AVStream *st = avformat_new_stream(formatCtx, NULL);
    AVStream *at  = avformat_new_stream(formatCtx, NULL);
    if (!st || !at)
        return;
    /***************************** Video ********************************/
    //h264 (libx264) ([7][0][0][0] / 0x0007), yuv420p(progressive), 640x480, q=-1--1, 59 fps, 1k tbn, 59 tbc
    /*add for new API begin*/
    st->codecpar->codec_id = AV_CODEC_ID_H264;
    st->codecpar->codec_type = AVMEDIA_TYPE_VIDEO;
    /*add for new API end*/
    st->codec->codec_id = AV_CODEC_ID_H264;
    st->codec->codec_type = AVMEDIA_TYPE_VIDEO;
    st->codec->width = 640;
    st->codec->height = 480;
    st->codec->ticks_per_frame = 1;
    st->codec->pix_fmt = AV_PIX_FMT_YUV420P;//AV_PIX_FMT_YUV420P;
    st->pts_wrap_bits = 32;
    st->codec->time_base.den = st->time_base.den = 1000;
    st->codec->time_base.num = st->time_base.num = 1;
    st->codec->sample_fmt = AV_SAMPLE_FMT_NONE;
    st->r_frame_rate.den = st->avg_frame_rate.den = 2;
    st->r_frame_rate.num = st->avg_frame_rate.num = 60;
    // H264 need sps/pps for decoding, so read it from the first video tag.
    printf("==========>begin find get_video_extradata: %0.3fs\n",(av_gettime()-start) / 1000000.0 );
    printf("get_video_extradata ret = %d\n",get_video_extradata(formatCtx, 0));
    printf("==========>after find get_video_extradata: %0.3fs\n",(av_gettime()-start) / 1000000.0 );
    formatCtx->pb->buf_ptr = formatCtx->pb->buf_end;
    formatCtx->pb->pos = (int64_t) formatCtx->pb->buf_end;

    /***************************** Audio ********************************/
    //Audio: aac (LC) ([10][0][0][0] / 0x000A), 44100 Hz, stereo, fltp, 128 kb/s
    /*add for new API begin*/
    at->codecpar->codec_id = AV_CODEC_ID_AAC;
    at->codecpar->codec_type = AVMEDIA_TYPE_AUDIO;
    at->codecpar->channels = 2;
    at->codecpar->channel_layout = 3;
    at->codecpar->bit_rate = 128000;
    at->codecpar->sample_rate = 44100;
    /*add for new API end*/
    at->codec->codec_id = AV_CODEC_ID_AAC;
    at->codec->codec_type = AVMEDIA_TYPE_AUDIO;
    at->codec->sample_rate = 44100;
    at->codec->time_base.den = 44100;
    at->codec->time_base.num = 1;
    at->codec->bits_per_coded_sample = 16; //
    at->codec->channels = 2;
    at->codec->channel_layout = 3;
    at->codec->bit_rate = 128000;//128000
    at->codec->refs = 1;
    at->codec->sample_fmt = AV_SAMPLE_FMT_FLTP;
    at->codec->profile = 1;
    at->codec->level = -99;
    at->pts_wrap_bits = 32;
    at->time_base.den = 1000;
    at->time_base.num = 1;
}

int _tmain(int argc, char* argv[])
{
    AVFormatContext *pFormatCtx;
    //Video
    AVCodecContext *pCodecCtx;
    AVCodec *pCodec;
    //Audio
    AVCodecContext *pACodecCtx;
    AVCodec *pACodec;

    AVFrame *pFrame,*pFrameYUV,*pAFrame;
    AVPacket *packet;
    struct SwsContext *img_convert_ctx;
    struct SwrContext *au_convert_ctx;

    int y_size;
    int ret,got_pic;
    int i,videoIndex,audioIndex;
    unsigned char* out_buffer;
    char filePath[] = "rtmp://192.168.1.201/live/mystream";
    //SDL begin
    int screen_w = 0,screen_h = 0;
    SDL_Window *screen;
    SDL_Renderer* sdlRenderer;
    SDL_Texture* sdlTexture;
    SDL_Rect sdlRect;
    SDL_Thread *play_tid;
    SDL_Event event;
    //add Audio
    SDL_AudioSpec audioSpec;
    //SDL end

    //------
    av_register_all();
    avdevice_register_all();
    avformat_network_init();

    pFormatCtx = avformat_alloc_context();
    //try to find device from window
    //get_dsshow_device(pFormatCtx);
    //AVInputFormat *pFmt = av_find_input_format("dshow");
    int64_t start = av_gettime();
    printf("==========>begin avformat_open_input : %0.3fs\n",(av_gettime()-start) / 1000000.0);

    if(avformat_open_input(&pFormatCtx,filePath,NULL,NULL))
        //if(avformat_open_input(&pFormatCtx,"video=e2eSoft VCam",pFmt,NULL) != 0)
    {
        printf("Couldn't open input stream.\n");
        return -1;
    }
    //pFormatCtx->probesize = 4096;

    printf("==========>begin find stream info 1: %0.3fs\n",(av_gettime()-start) / 1000000.0 );
    if(1){
        //init_Stream1(pFormatCtx,start);
        init_Stream2(pFormatCtx,start);
    }else{
        if(avformat_find_stream_info(pFormatCtx,NULL) < 0)
        {
            printf("Couldn't find stream information.\n");
            return -1;
        }
    }
    printf("==========>after find stream info :%0.3fs\n",(av_gettime()-start) / 1000000.0 );

    printf("----------------File Information----------------\n");
    av_dump_format(pFormatCtx,0,filePath,0);
    printf("------------------------------------------------\n");

    videoIndex = audioIndex = -1;

    for(i = 0; i < pFormatCtx->nb_streams; i++)
    {
        if(pFormatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO)
        {
            videoIndex = i;
            break;
        }
    }

    for(i = 0; i < pFormatCtx->nb_streams; i++)
    {
        if(pFormatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_AUDIO)
        {
            audioIndex = i;
            break;
        }
    }

    if(videoIndex == -1 || audioIndex == -1)
    {
        printf("Couldn't find video|audio stream.\n");
        return -1;
    }

    pCodecCtx = pFormatCtx->streams[videoIndex]->codec;
    pCodec = avcodec_find_decoder(pCodecCtx->codec_id);
    pACodecCtx = pFormatCtx->streams[audioIndex]->codec;
    pACodec = avcodec_find_decoder(pACodecCtx->codec_id);

    if(pCodec == NULL)
    {
        printf("Codec not found.\n");
        return -1;
    }

    if(avcodec_open2(pCodecCtx,pCodec,NULL)< 0 || avcodec_open2(pACodecCtx,pACodec,NULL)< 0)
    {
        printf("Could not open codec.\n");
        return -1;
    }

    packet = (AVPacket *)av_malloc(sizeof(AVPacket));
    av_init_packet(packet);
    pFrame = av_frame_alloc();
    pAFrame = av_frame_alloc();

    //Video 
    pFrameYUV = av_frame_alloc();
    out_buffer =  (unsigned char*)av_malloc(av_image_get_buffer_size(AV_PIX_FMT_YUV420P,pCodecCtx->width,pCodecCtx->height,1));
    av_image_fill_arrays(pFrameYUV->data,pFrameYUV->linesize,out_buffer,AV_PIX_FMT_YUV420P,pCodecCtx->width,pCodecCtx->height,1);
    img_convert_ctx = sws_getContext(pCodecCtx->width,pCodecCtx->height,pCodecCtx->pix_fmt,
        pCodecCtx->width,pCodecCtx->height,AV_PIX_FMT_YUV420P,SWS_BICUBIC,NULL,NULL,NULL);
    //Out params for Audio
    uint64_t out_channel_layout = AV_CH_LAYOUT_STEREO;
    int out_nb_samples = pACodecCtx->frame_size;
    AVSampleFormat out_sample_fmt = AV_SAMPLE_FMT_S16;
    int out_sample_rate = 44100;
    int out_channels = av_get_channel_layout_nb_channels(out_channel_layout);
    int out_buffer_size = av_samples_get_buffer_size(NULL,out_channels,out_nb_samples,out_sample_fmt,1);
    out_buffer = (uint8_t *)av_malloc(MAX_AUDIO_FRAME_SIZE*2);

    //SDL begin
    if(SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO | SDL_INIT_TIMER))
    {
        printf("Could not initialize SDL - %s\n",SDL_GetError());
        return -1;
    }

    screen_w = pCodecCtx->width;
    screen_h = pCodecCtx->height;
    screen = SDL_CreateWindow("Simple ffmpeg player's window",SDL_WINDOWPOS_UNDEFINED,SDL_WINDOWPOS_UNDEFINED,screen_w,screen_h,SDL_WINDOW_OPENGL);

    if(!screen)
    {
        printf("SDL: could not create window - exiting:%s\n",SDL_GetError());    
        return -1;
    }

    sdlRenderer = SDL_CreateRenderer(screen,-1,0);
    //IYUV: Y + U + V
    //YV12: Y + V + U
    sdlTexture = SDL_CreateTexture(sdlRenderer,SDL_PIXELFORMAT_IYUV,SDL_TEXTUREACCESS_STREAMING,screen_w,screen_h);
    sdlRect.x = 0;
    sdlRect.y = 0;
    sdlRect.w = screen_w;
    sdlRect.h = screen_h;

    //for audio
    audioSpec.freq = out_sample_rate;
    audioSpec.format = AUDIO_S16SYS;
    audioSpec.channels = out_channels;
    audioSpec.silence = 0;
    audioSpec.samples = out_nb_samples;
    audioSpec.callback = fill_audio;
    audioSpec.userdata = pACodecCtx;

    if(SDL_OpenAudio(&audioSpec,NULL) < 0)
    {
        printf("cant's  open audio.\n");
        return -1;
    }
    play_tid = SDL_CreateThread(sfp_refresh_thread,NULL,NULL);
    //SDL end

    //FIX:Some Codec's Context Information is missing  
    int64_t in_channel_layout=av_get_default_channel_layout(pACodecCtx->channels);  
    //Swr
    au_convert_ctx = swr_alloc();
    au_convert_ctx = swr_alloc_set_opts(au_convert_ctx,
        out_channel_layout,out_sample_fmt,out_sample_rate,
        in_channel_layout,pACodecCtx->sample_fmt,pACodecCtx->sample_rate
        ,0,NULL);
    swr_init(au_convert_ctx);

    //Play
    SDL_PauseAudio(0);

    //EventLoop
    for(;;)
    {
        //Wait
        SDL_WaitEvent(&event);
        if(event.type == SFM_REFRESH_EVENT)
        {

            if(av_read_frame(pFormatCtx,packet) < 0)
            {
                thread_exit = 1;
            }
            if(packet->stream_index == videoIndex){
                ret = avcodec_decode_video2(pCodecCtx,pFrame,&got_pic,packet);
                if(ret < 0)
                {
                    printf("Decode Error.\n");
                    return -1;
                }
                if(got_pic)
                {
                    sws_scale(img_convert_ctx,pFrame->data,pFrame->linesize,
                        0,pCodecCtx->height,pFrameYUV->data,pFrameYUV->linesize);

                    //SDL begin
                    SDL_UpdateTexture(sdlTexture,NULL,pFrameYUV->data[0],pFrameYUV->linesize[0]);
                    SDL_RenderClear(sdlRenderer);
                    SDL_RenderCopy(sdlRenderer,sdlTexture,NULL,&sdlRect);
                    SDL_RenderPresent(sdlRenderer);
                }
            }else if(packet->stream_index == audioIndex)
            {
                ret = avcodec_decode_audio4(pACodecCtx,pAFrame,&got_pic,packet);
                if(ret < 0)
                {
                    printf("Decode audio Error.\n");
                    return -1;
                }
                if(got_pic)
                {
                    swr_convert(au_convert_ctx,&out_buffer,MAX_AUDIO_FRAME_SIZE,(const uint8_t **)pAFrame->data,pAFrame->nb_samples);
                    audio_chunk = (Uint8  *)out_buffer;
                    audio_len = out_buffer_size;
                    audio_pos = audio_chunk;

                }
            }
            av_free_packet(packet);
        }else if(event.type == SDL_KEYDOWN)
        {
            //Pause
            if(event.key.keysym.sym == SDLK_SPACE)
            {
                thread_pause = !thread_pause;
            }
        }else if(event.type == SDL_QUIT)
        {
            thread_exit = 1;
        }else if(event.type == SFM_BREAK_EVENT)
        {
            break;
        }
    }

    sws_freeContext(img_convert_ctx);
    swr_free(&au_convert_ctx);

    SDL_CloseAudio();
    SDL_Quit();

    av_frame_free(&pFrameYUV);
    av_frame_free(&pFrame);
    av_frame_free(&pAFrame);
    avcodec_close(pCodecCtx);
    avcodec_close(pACodecCtx);
    avformat_close_input(&pFormatCtx);

    return 0;
}
```
